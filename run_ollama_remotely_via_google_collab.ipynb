{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yanniedog/autocrew/blob/main/run_ollama_remotely_via_google_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YI9Cxw8PyWN0"
      },
      "outputs": [],
      "source": [
        "# filename: run_ollama_remotely_via_google_collab.ipynb\n",
        "\n",
        "# Install pyngrok\n",
        "!pip install pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    # Terminate any existing ngrok sessions\n",
        "    ngrok.kill()\n",
        "\n",
        "    # Retrieve your ngrok auth token from the Colab secret storage\n",
        "    auth_token = userdata.get('authtoken')\n",
        "    if not auth_token:\n",
        "        raise ValueError(\"Ngrok auth token not found in Colab secrets.\")\n",
        "\n",
        "    # Set the ngrok authentication token programmatically\n",
        "    ngrok.set_auth_token(auth_token)\n",
        "\n",
        "    # Check if Ollama is installed; if not, install it\n",
        "    try:\n",
        "        ollama_installed = subprocess.run([\"ollama\", \"--version\"], capture_output=True, text=True)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Ollama is not installed. Installing Ollama...\")\n",
        "\n",
        "        # Install Ollama using the provided command\n",
        "        install_ollama = subprocess.run(\"curl https://ollama.ai/install.sh | sh\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "        # Check if Ollama installation was successful\n",
        "        ollama_installed = subprocess.run([\"ollama\", \"--version\"], capture_output=True, text=True)\n",
        "\n",
        "    if ollama_installed.returncode == 0:\n",
        "        print(\"Ollama installation was successful.\")\n",
        "\n",
        "        # Start an HTTP tunnel on the desired port, e.g., 11434\n",
        "        tunnel = ngrok.connect(11434)\n",
        "        print(\"Ngrok Tunnel URL:\", tunnel.public_url)\n",
        "\n",
        "        # Run Ollama serve\n",
        "        ollama_process = subprocess.Popen([\"ollama\", \"serve\"])\n",
        "        print(\"Ollama serve process started.\")\n",
        "    else:\n",
        "        print(\"Ollama installation failed.\")\n",
        "        raise Exception(\"Ollama installation failed.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"An error occurred:\", str(e))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1X51OeU2ojRCrja4JYsinWCrGGkCsYdp1",
      "authorship_tag": "ABX9TyOfsiZHrn74I5FfA8CAEO/V",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}